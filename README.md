# Self-Supervised Learning Library
This is a personal repo for fast implementation of various self-supervised learning algorithms.

## Currently Supported Models
- BYOL, [https://arxiv.org/abs/2006.07733](https://arxiv.org/abs/2006.07733)
- RAFT, [https://arxiv.org/abs/2006.07733](https://arxiv.org/abs/2011.10944)
- modified-Whitening 

## Deploying Models

### Training
Training on CIFAR10, need to set resize-dim to 32. However on Imagenet or SubImagenet, need to set it to 224.

(SubImagenet is a 100-class imagenet dataset, refer to [https://github.com/HobbitLong/CMC/issues/21](https://github.com/HobbitLong/CMC/issues/21) for more information.)
```
nohup python -m train.train --dataset=subimagenet --model=byol --encoder=resnet50 --projector=2layermlpbn --predictor=2layermlpbn --mlp-middim=4096 --mlp-outdim=256 --normalization=l2 --gpus=8 --port=8010 --num-workers=8 --epochs=300 --batch-size=128 --rand-seed=2333 --resize-dim=224 --optimizer=lars --lr=0.3 --ema-lr=1e-2 --ema-mode=sgd --checkpoint-epochs=10 --amp=False --sync-bn=True --checkpoint-dir=checkpoints/tmp --weight-wrapper=emacosine --log=True --logger-wrappers=alignlogger.uniformlogger.prednormlogger --weight-decay=1e-6 --lr-wrapper=lr.linearcosineladder --warmup-epochs=2 --ladder-epochs=40 --ladder-scalers=0.1
```

### Evaluation
#### Linear evlauation protocol
Implemented in `eval/eval.py`. It could be slow if evaluated on the full imagenet dataset. 
```
nohup python -m eval.eval --dataset=imagenet --encoder=resnet18 --gpu=3 --epochs=200 --batch-size=40 --resize-dim=32 --eval-mode=online --checkpoint=checkpoint/path/ &
```

#### Fast evaluation using knn
Implemented in `eval/knn.py`. It speeds up the evaluation process by freezing the encoder network and
pre-computing the features generated by the encoder network.

```
python -m eval.knn --dataset=imagenet --encoder=resnet50 --gpus=4 --batch-size=256 --resize-dim=224 --eval-mode=whole --checkpoint=checkpoints/byol_res50_reference.pt --num-workers=4 --k=5
```

## Customization
### Preparing Data
You can store the new data folder in the `/data` directory and create a python file implementing three methods loading the data.
This repo finds the data loader by its name and you can pass the dataset name by the command line argument `--dataset={dataset_name}`.

### Customize data augmentation
If you are dealing with the CV task, you can follow the `/models/submodels/augmentations.py` to customize your own data augmentation method. 
In general, we provide the registry mechanism and you can specify the augmentation type by its registered name by the command line argument `--aug={aug_name}`. 
In the file `train/trian.py`, we invoke the augmentation by `trans = AUGS[{aug_name}]`. 

### Customize the main model
The main model is basically a `nn.Module` class, please follow the `/models/main/byol.py` to customize your own SSL model. The `register_reps` method register some of the representations for the `logger_wrapper`. It is known that the `alignlogger` and `uniformlogger` are crucial to the performance of BYOL.

The encoder network in the SSL model `model` is named as `model.online` following the BYOL naming protocol. This protocol can be broken, but remember to modify the `load_model` function in `/eval/utils.py` if you do so.

### Inspect more statistics than Alignment and Uniformity
You can follow the rule of `/models/wrappers/logger_wrapper.py` to customize the inspection over the model during training. `prednormlogger` is a perfect example of inspecting the BYOL's predictor's weight norm.

### Learning rate scheduling
It's is written in the `/models/wrappers/lr_wrapper.py`. Currently we support warm-up and annealing (plus ladder scheduling).