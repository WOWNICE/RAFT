# Self-Supervised Learning Library
This is a personal repo for fast implementation of various self-supervised learning algorithms.

## Currently Supported Models
- BYOL, [https://arxiv.org/abs/2006.07733](https://arxiv.org/abs/2006.07733)
- RAFT, [https://arxiv.org/abs/2006.07733](https://arxiv.org/abs/2011.10944)
- modified-Whitening 

## Training
Training on CIFAR10, need to set resize-dim to 32. However on Imagenet or SubImagenet, need to set it to 224.

(SubImagenet is a 100-class imagenet dataset, refer to [https://github.com/HobbitLong/CMC/issues/21](https://github.com/HobbitLong/CMC/issues/21) for more information.)
```
CUDA_VISIBLE_DEVICES=0,1 nohup python -m train.train --dataset=cifar10 \
--model={raft/byol/whitening} --encoder=resnet50 --projector=2layermlpbn --predictor=linear \
--mlp-middim=512 --mlp-outdim=256 --normalization=l2 --gpus=2 --port=8010 --num-workers=1 \
--epochs=1000 --batch-size=512 --rand-seed=2333 --resize-dim=32 --optimizer=adam --lr=3e-4 \
--ema-lr=4e-3 --ema-mode=sgd --checkpoint-epochs=50 --amp=False --sync-bn=False \
--checkpoint-dir=checkpoint/path/ --weight-wrapper=ema --logger-wrappers=uniformlogger.alignlogger &
```

## Evaluation
### Linear evlauation protocol
Implemented in `eval/eval.py`. It could be slow if evaluated on the full imagenet dataset. 
```
nohup python -m eval.eval --dataset=imagenet --encoder=resnet18 --gpu=3 --epochs=200 \
--batch-size=40 --resize-dim=32 --eval-mode=online --checkpoint=checkpoint/path/ &
```

### Fast linear evaluation protocol
Implemented in `eval/fast_eval.py`. It speeds up the evaluation process by freezing the encoder network and
pre-computing the features generated by the encoder network.

```
nohup python -m eval.fast_eval --dataset=imagenet --encoder=resnet18 --gpu=3 --epochs=200 \
--batch-size=40 --resize-dim=32 --eval-mode=online --checkpoint=checkpoint/path/ &
```